Notes: 
think of different critertions which part of the network to update(maybe highest grad norm or smthn)
does the armijo criterion i.e. make sense for only a part of the network? maybe use the loss of the subnetwork (i.e. the gradient?) maybe by integrating the gradient on the input part of the subnetwork
understand complexity and see what it relates to in a transformer
SLS + SAM on the network
try 10 adamsls on transformer pre-training
maybe try radam instead of adam
TODO: find a use case for PLASLS, maybe image classification with resnet
When taking a step in the future only take a step with the updated part of the network, freeze the rest

5 runs * 6 optimizers * 12 datasets = ~ 100 days of compute on a A40 GPU
